---
title: "A3: Classification"
author: "Credibles: Menghong Han, Peihan Tian, Yanghe Liu, Laurence Finch,"
date: "12 March 2020"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---
# Part 1

# Import data
```{r, warning=FALSE, message=FALSE}
set.seed(1)
rm(list=ls()); gc()
library(lubridate)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(GGally)

library(tidyr)
library(rpart)
library(rpart.plot)
library(caret)
library(e1071)
library(factoextra)
library(randomForest)
library(pROC)
library(factoextra)
data<-read.csv('dataset2.csv')
```






# Transform continous target variable SMALL.HYDRO into categorical variable with 3 levels according to K-means result
```{r}
data<-data[,-c(1,2,3)]
data$SH=data$SMALL.HYDRO
data$SH[data$SMALL.HYDRO<238]=0
data$SH[data$SMALL.HYDRO>402]=2
data$SH[data$SMALL.HYDRO<403&data$SMALL.HYDRO>237]=1

```

# Boxplot for important variables
```{r}
# Box & whisker plots for relevant and important variables
boxplot(data)
boxplot(data$SMALL.HYDRO)
```


# Calculate descriptive statistics
```{r}

getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

summary(data$SMALL.HYDRO)
getmode(data$SMALL.HYDRO)
sd(data$SMALL.HYDRO)
var(data$SMALL.HYDRO)
```
# Scatterplot & correlations
```{r}
# sampling 100 rows from dataset to get clear scatterplot
length_data=dim(data)[1]
index<-sample(c(1:length_data),100)
selected.var=c(1:9) # select the meaningful variables from dataset
dataplot=data[index,selected.var] 

# scatterplot 
ggpairs(dataplot,lower=list(continuous=wrap("smooth", colour="lightblue")))
```

# Part 2 Classification Modeling
First, after loading our required libraries, we read in the data and divide it into training (80% of dataset) and validation sets.

Small hydro energy production in California is defined as energy production from water related sources, at a facility with a capacity of 30MW or less. More information about small hydro can be found [here](https://www.hydro.org/policy/technology/small-hydro/).

We are interested in how the level of small hydro energy production can be predicted using the other type of renewable energy and some time and month indicator variable.

#clustering
```{r}
dataset<-read.csv('dataset2.csv')
km_result=kmeans(dataset$SMALL.HYDRO,3)
print(km_result$centers)

dataset$cut=km_result$cluster
dataset%>%group_by(cut)%>%summarize(mean_size=max(SMALL.HYDRO,na.rm = TRUE))

dataset$target_variable=cut(dataset$SMALL.HYDRO,breaks = c(0,236,402,678),labels = c('low','medium','high'))

dataset$SMALL.HYDRO=NULL
dataset$cut=NULL
dataset$SOLAR.PV_.3=NULL

selected.var=c(4:dim(dataset)[2])
dataset=dataset[complete.cases(dataset$target_variable),]
```



# divide the dataset into training and testing parts
```{r}
length_dataset=dim(dataset)[1]*0.8
train.index=sample(c(1:length_dataset),35921)
train.df=dataset[train.index,selected.var]
valid.df=dataset[-train.index,selected.var]

```

# Model 1-1: Classification tree
```{r}
default.ct <- rpart(target_variable ~ ., data = train.df, method = "class")
# plot tree
prp(default.ct, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)


# matrix 1
default.ct.point.pred.valid <- predict(default.ct,valid.df,type = "class")
default.ct.point.pred.train = predict(default.ct,train.df,type = 'class')
confusionMatrix(default.ct.point.pred.valid, as.factor(valid.df$target_variable))

#accuracy in the training
conf_nat_tree_train=table(train.df$target_variable,default.ct.point.pred.train)
(sum(diag(conf_nat_tree_train))/sum(conf_nat_tree_train)*100)

#accuracy in the testing 
conf_nat_tree_valid=table(valid.df$target_variable,default.ct.point.pred.valid)
(sum(diag(conf_nat_tree_valid))/sum(conf_nat_tree_valid)*100)

```

# Model 1-2: Deeper trees
```{r}
deeper.ct <- rpart(target_variable ~ ., data = train.df, method = "class", cp = 0, minsplit = 1)
# count number of leaves
length(deeper.ct$frame$var[deeper.ct$frame$var == "<leaf>"])
# plot tree
prp(deeper.ct, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10, 
    box.col=ifelse(deeper.ct$frame$var == "<leaf>", 'gray', 'white'))  

# matrix 2
deeper.ct.point.pred.train <- predict(deeper.ct,train.df,type = "class")
confusionMatrix(deeper.ct.point.pred.valid, as.factor(valid.df$target_variable))

#accuracy in the training
deeper.ct.point.pred.valid <- predict(deeper.ct,valid.df,type = "class")
conf_nat_tree_train=table(train.df$target_variable,deeper.ct.point.pred.train)
(sum(diag(conf_nat_tree_train))/sum(conf_nat_tree_train)*100)

#accuracy in the testing 
conf_nat_tree_valid=table(valid.df$target_variable,deeper.ct.point.pred.valid)
(sum(diag(conf_nat_tree_valid))/sum(conf_nat_tree_valid)*100)

```

# Model 1-3: Random forest

```{r}
train.df$target_variable=as.factor(train.df$target_variable)
valid.df$target_variable=as.factor(valid.df$target_variable)
forest= randomForest(target_variable~.,data = train.df,ntree=500,mtry=20,importance=TRUE,na.action = na.exclude,nodesize=5)
forest$importance
varImpPlot(forest, main = "variable importance")

pre_ran=predict(forest,newdata = valid.df)
pre_ran2=predict(forest,newdata = train.df)
obs_p_ran=data.frame(prob=pre_ran,obs=valid.df$target_variable)
conf_nat=table(valid.df$target_variable,pre_ran)
conf_nat_2=table(train.df$target_variable,pre_ran2)

(sum(diag(conf_nat_2))/sum(conf_nat_2)*100)
(Accuracy <-sum(diag(conf_nat))/sum(conf_nat)*100)

#matrix 3
matrix1=confusionMatrix(pre_ran, as.factor(valid.df$target_variable))
matrix1$byClass

```




# Model 2: Logistic Regression
```{r}
library(nnet)
library(caret)
train.df$target_variable=relevel(train.df$target_variable, ref = "medium")
multinom.fit=multinom(target_variable~.-1,data = train.df)
summary(multinom.fit)
exp(coef(multinom.fit))
probability.table=fitted(multinom.fit)

#training set
train.predict=predict(multinom.fit,newdata = train.df,'class')
ctable=table(train.df$target_variable,train.predict)
round((sum(diag(ctable))/sum(ctable))*100,2)

#testing set
test_predict=predict(multinom.fit,newdata = valid.df,'class')
ctable=table(valid.df$target_variable,test_predict)
round((sum(diag(ctable))/sum(ctable))*100,2)

#find p value and z score
output=summary(multinom.fit)
z=output$coefficients/output$standard.errors
p <- (1 - pnorm(abs(z), 0, 1))*2 # we are using two-tailed z test
Pclass2 <- rbind(output$coefficients[1,],output$standard.errors[1,],z[1,],p[1,])
rownames(Pclass2) <- c("Coefficient","Std. Errors","z stat","p value")
knitr::kable(Pclass2)

Pclass3 <- rbind(output$coefficients[2,],output$standard.errors[2,],z[2,],p[2,])
rownames(Pclass3) <- c("Coefficient","Std. Errors","z stat","p value")
knitr::kable(Pclass3)
```

### remove outliers
```{r}
#we divide the dataset into training and testing parts
set.seed(1)
dataset=read.csv('dataset2.csv')
dataset <- dataset[-c(30896, 28678,811,890,271,31345,30895,6835,4292,27890,170,27170),]
length_dataset=dim(dataset)[1]*0.8
train.index=sample(c(1:length_dataset),35913)


#clustering 
km_result=kmeans(dataset$SMALL.HYDRO,3)
print(km_result$centers)

dataset$cut=km_result$cluster
dataset%>%group_by(cut)%>%summarize(mean_size=max(SMALL.HYDRO,na.rm = TRUE))

dataset$target_variable=cut(dataset$SMALL.HYDRO,breaks = c(0,236,402,678),labels = c('low','medium','high'))

dataset$SMALL.HYDRO=NULL
dataset$cut=NULL
dataset$SOLAR.PV_.3=NULL

selected.var=c(4:dim(dataset)[2])
dataset=dataset[complete.cases(dataset$target_variable),]




# divide the dataset into training and testing parts
```{r}
length_dataset=dim(dataset)[1]*0.8
train.index=sample(c(1:length_dataset),35913)
train.df=dataset[train.index,selected.var]
valid.df=dataset[-train.index,selected.var]


train.df$target_variable=relevel(train.df$target_variable, ref = "medium")
multinom.fit=multinom(target_variable~.-1,data = train.df)
summary(multinom.fit)
exp(coef(multinom.fit))
probability.table=fitted(multinom.fit)

#training set
train.predict=predict(multinom.fit,newdata = train.df,'class')
ctable=table(train.df$target_variable,train.predict)
round((sum(diag(ctable))/sum(ctable))*100,2)

#testing set
test_predict=predict(multinom.fit,newdata = valid.df,'class')
ctable=table(valid.df$target_variable,test_predict)
round((sum(diag(ctable))/sum(ctable))*100,2)

#find p value and z score
output=summary(multinom.fit)
z=output$coefficients/output$standard.errors
p <- (1 - pnorm(abs(z), 0, 1))*2 # we are using two-tailed z test
Pclass2 <- rbind(output$coefficients[1,],output$standard.errors[1,],z[1,],p[1,])
rownames(Pclass2) <- c("Coefficient","Std. Errors","z stat","p value")
knitr::kable(Pclass2)

Pclass3 <- rbind(output$coefficients[2,],output$standard.errors[2,],z[2,],p[2,])
rownames(Pclass3) <- c("Coefficient","Std. Errors","z stat","p value")
knitr::kable(Pclass3)        

```

# Model 3: KNN

```{r}
library(class)
library(gmodels)
library(psych)
library(caret)
library(FNN)
data<-read.csv('classification.csv')
# dropping useless variables
data <- data[-c(1:4)]
```

# Normalizing function and normalizing 
```{r}
normalize = function(x){return ((x - min(x)) / (max(x) - min(x)))}
data_n    = as.data.frame(lapply(data[,1:65], normalize))
head(data_n)
```

# create training and test data
```{r}
set.seed(111)
train.index <- sample(row.names(data_n), 0.8*dim(data_n)[1])  
valid.index <- setdiff(row.names(data_n), train.index)  
train.df <- data_n[train.index, ]
valid.df <- data_n[valid.index, ]

# create labels for training and test data
data_train_label <- data[train.index, 66]
data_valid_label <- data[valid.index, 66]

# initialize a data frame with two columns: k, and accuracy.
accuracy.df <- data.frame(k = seq(1, 14, 1), accuracy = rep(0, 14))
```

# compute knn for different k on validation.
```{r}
for(i in 1:14) {
  data_valid_pred = class::knn(train = train.df, 
                               cl    = data_train_label,
                               test  = valid.df,
                               k     = i)
  accuracy.df[i, 2] <- confusionMatrix(data_valid_pred, data_valid_label)$overall[1]
}

accuracy.df
plot(accuracy.df)

```

From the plot, we see that when k = 3, we have the highest accuracy, and the accuracy is around 80%. 
So we make k equals 3 and calculate again

# k = 3 
```{r}
data_valid_pred = class::knn(train = train.df, 
                             cl    = data_train_label,
                             test  = valid.df,
                             k     = 3)

# Use confusionmatrix to check the characteristics of prediction
confusionmatrix <- confusionMatrix(data_valid_pred, data_valid_label)
characteristics <- confusionmatrix$byClass

# Compare in-sample accuracy and out-of-sample accuracy
data_train_pred = class::knn(train = train.df, 
                             cl    = data_train_label,
                             test  = train.df,
                             k     = 3)

In_sample_accuracy <- confusionMatrix(data_train_pred, data_train_label)$overall[1]
```

It shows that KNN has an in-sample accuracy of 89.39%, and an out-of-sample accuracy of 79.76%
